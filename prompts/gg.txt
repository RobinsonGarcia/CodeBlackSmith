### __init__.py ###

### __init__.py ###
from .common.assistant_registry import AssistantRegistry
from . import cli, common

__all__ = [AssistantRegistry, cli, common] 

### cli.py ###
import sys
import subprocess
import uuid
from .common import AssistantRegistry
from langchain_ollama import ChatOllama
import ollama

#############################
# Assistant Module Utilities#
#############################
class ModelNotFoundError(Exception):
    """Custom exception to handle missing Ollama models."""
    pass

def check_if_model_exists(model_name):
    available_models = [m["model"] for m in ollama.list()["models"]]
    if model_name in available_models:
        return True
    else:
        error_message = (
            f"‚ùå Model '{model_name}' not found in Ollama.\n"
            f"üì• To download it, run:\n\n"
            f"   ollama pull {model_name}\n"
            f"\nüîπ Available models: {', '.join(available_models) if available_models else 'None'}"
        )
        raise ModelNotFoundError(error_message)

#########################
# Assistant CLI Handlers#
#########################
def start_assistant(model_name, base_dir, temperature, num_ctx):
    try:
        check_if_model_exists(model_name)
        print("‚úÖ Model is available.")
    except ModelNotFoundError as e:
        print(e, file=sys.stderr)
        sys.exit(1)
    llm = ChatOllama(model=model_name, temperature=temperature, num_ctx=num_ctx)
    assistant_name = "react_assistant"
    if assistant_name not in AssistantRegistry.list_assistants():
        print(f"Error: Assistant '{assistant_name}' is not available.", file=sys.stderr)
        sys.exit(1)
    agent = AssistantRegistry.get_assistant(assistant_name, llm, base_dir)
    print(f"üîπ Running '{assistant_name}' assistant with Ollama model: {model_name}")
    print("üí¨ Type your messages below. Type 'exit' to quit.\n")
    thread_id = str(uuid.uuid4())
    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üîª Exiting assistant.")
            break
        try:
            inputs = {"messages": [("user", user_input)]}
            config = {"configurable": {"thread_id": thread_id}}
            response = agent.invoke(inputs, config=config)
            if isinstance(response, dict) and "messages" in response:
                print(f"Assistant: {response['messages'][-1].content}")
            else:
                print(f"Assistant: {response.content}")
        except Exception as e:
            print(e, file=sys.stderr)

import subprocess

def start_server(model_name, base_dir):
    print(f"üöÄ Starting web server with model '{model_name}' and base directory '{base_dir}'...")
    subprocess.run(["python", "-m", "codepromptforge.assistant.web_assistant.app", "--model", model_name, "--base-dir", base_dir])

###############################
# Assistant Commands Registration
###############################
def register_commands(subparsers):
    # Register CLI assistant command
    parser_assistant = subparsers.add_parser("cli_assistant", help="Run the advanced assistant CLI")
    parser_assistant.add_argument("--model", required=True, help="Ollama model to use (e.g., llama3.3, qwen2.5:14b)")
    parser_assistant.add_argument("--base-dir", required=True, help="Base directory for assistant operations")
    parser_assistant.add_argument("--temperature", type=float, default=0.0, help="Temperature setting for the model")
    parser_assistant.add_argument("--num_ctx", type=int, default=80000, help="Context length for the model")
    parser_assistant.set_defaults(func=handle_assistant)

    # Register web assistant command
    parser_web = subparsers.add_parser("web_assistant", help="Start the advanced web assistant server")
    parser_web.add_argument("--model", required=True, help="Ollama model to use")
    parser_web.add_argument("--base-dir", required=True, help="Base directory for assistant operations")
    parser_web.set_defaults(func=handle_web)

def handle_assistant(args):
    start_assistant(args.model, args.base_dir, args.temperature, args.num_ctx)

def handle_web(args):
    start_server(args.model, args.base_dir)
### cli.py ###
import argparse
import sys
import subprocess
import codehammer  # Import CodeHammer CLI commands
from .assistant.cli import register_commands as register_assistant_commands

#######################
# Command Forwarding  #
#######################
def forward_to_codehammer():
    """Forward unknown commands to CodeHammer."""
    command = ["codehammer"] + sys.argv[1:]
    result = subprocess.run(command, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print(result.stderr, file=sys.stderr)
    sys.exit(result.returncode)

#######################
# Main CLI Entry Point#
#######################
def main():
    parser = argparse.ArgumentParser(description="CodeBlacksmith: AI & Code Management CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Register CodeHammer CLI commands
    codehammer.cli.register_core_commands(subparsers)

    # Register Assistant Commands
    register_assistant_commands(subparsers)

    args, unknown_args = parser.parse_known_args()

    # If the command is from CodeHammer, forward it
    if args.command in ["tree", "file", "files", "files_recursive", "write", "combine", "clean_result"]:
        forward_to_codehammer()

    if hasattr(args, "func"):
        args.func(args)
    else:
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()
### __init__.py ###

### __init__.py ###
from .common.assistant_registry import AssistantRegistry
from . import cli, common

__all__ = [AssistantRegistry, cli, common] 

### cli.py ###
import sys
import subprocess
import uuid
from .common import AssistantRegistry
from langchain_ollama import ChatOllama
import ollama

#############################
# Assistant Module Utilities#
#############################
class ModelNotFoundError(Exception):
    """Custom exception to handle missing Ollama models."""
    pass

def check_if_model_exists(model_name):
    available_models = [m["model"] for m in ollama.list()["models"]]
    if model_name in available_models:
        return True
    else:
        error_message = (
            f"‚ùå Model '{model_name}' not found in Ollama.\n"
            f"üì• To download it, run:\n\n"
            f"   ollama pull {model_name}\n"
            f"\nüîπ Available models: {', '.join(available_models) if available_models else 'None'}"
        )
        raise ModelNotFoundError(error_message)

#########################
# Assistant CLI Handlers#
#########################
def start_assistant(model_name, base_dir, temperature, num_ctx):
    try:
        check_if_model_exists(model_name)
        print("‚úÖ Model is available.")
    except ModelNotFoundError as e:
        print(e, file=sys.stderr)
        sys.exit(1)
    llm = ChatOllama(model=model_name, temperature=temperature, num_ctx=num_ctx)
    assistant_name = "react_assistant"
    if assistant_name not in AssistantRegistry.list_assistants():
        print(f"Error: Assistant '{assistant_name}' is not available.", file=sys.stderr)
        sys.exit(1)
    agent = AssistantRegistry.get_assistant(assistant_name, llm, base_dir)
    print(f"üîπ Running '{assistant_name}' assistant with Ollama model: {model_name}")
    print("üí¨ Type your messages below. Type 'exit' to quit.\n")
    thread_id = str(uuid.uuid4())
    while True:
        user_input = input("User: ")
        if user_input.lower() in ["exit", "quit"]:
            print("üîª Exiting assistant.")
            break
        try:
            inputs = {"messages": [("user", user_input)]}
            config = {"configurable": {"thread_id": thread_id}}
            response = agent.invoke(inputs, config=config)
            if isinstance(response, dict) and "messages" in response:
                print(f"Assistant: {response['messages'][-1].content}")
            else:
                print(f"Assistant: {response.content}")
        except Exception as e:
            print(e, file=sys.stderr)

import subprocess

def start_server(model_name, base_dir):
    print(f"üöÄ Starting web server with model '{model_name}' and base directory '{base_dir}'...")
    subprocess.run(["python", "-m", "codepromptforge.assistant.web_assistant.app", "--model", model_name, "--base-dir", base_dir])

###############################
# Assistant Commands Registration
###############################
def register_commands(subparsers):
    # Register CLI assistant command
    parser_assistant = subparsers.add_parser("cli_assistant", help="Run the advanced assistant CLI")
    parser_assistant.add_argument("--model", required=True, help="Ollama model to use (e.g., llama3.3, qwen2.5:14b)")
    parser_assistant.add_argument("--base-dir", required=True, help="Base directory for assistant operations")
    parser_assistant.add_argument("--temperature", type=float, default=0.0, help="Temperature setting for the model")
    parser_assistant.add_argument("--num_ctx", type=int, default=80000, help="Context length for the model")
    parser_assistant.set_defaults(func=handle_assistant)

    # Register web assistant command
    parser_web = subparsers.add_parser("web_assistant", help="Start the advanced web assistant server")
    parser_web.add_argument("--model", required=True, help="Ollama model to use")
    parser_web.add_argument("--base-dir", required=True, help="Base directory for assistant operations")
    parser_web.set_defaults(func=handle_web)

def handle_assistant(args):
    start_assistant(args.model, args.base_dir, args.temperature, args.num_ctx)

def handle_web(args):
    start_server(args.model, args.base_dir)
### __init__.py ###
from .assistant_registry import AssistantRegistry
from . import react_assistant

__all__ = ["AssistantRegistry"]
### assistant_registry.py ###
from typing import Dict, Callable

class AssistantRegistry:
    """
    A registry for managing and retrieving assistant implementations dynamically.
    Each assistant must be registered with a unique name and a function that 
    builds the assistant using an LLM model.
    """

    _registry: Dict[str, Callable] = {}

    @classmethod
    def register_assistant(cls, name: str, builder: Callable):
        """
        Register an assistant with a unique name.

        Args:
            name (str): Unique identifier for the assistant.
            builder (Callable): A function that takes an LLM instance and returns the assistant.

        Raises:
            ValueError: If an assistant with the same name is already registered.
        """
        if name in cls._registry:
            raise ValueError(f"Assistant '{name}' is already registered.")
        cls._registry[name] = builder

    @classmethod
    def get_assistant(cls, name: str, llm, base_dir='..'):
        """
        Retrieve and build an assistant by name.

        Args:
            name (str): The name of the registered assistant.
            llm: The LLM model instance to be used by the assistant.

        Returns:
            The built assistant instance.

        Raises:
            KeyError: If the requested assistant is not found.
        """
        if name not in cls._registry:
            raise KeyError(f"Assistant '{name}' is not registered.")
        return cls._registry[name](llm, base_dir)

    @classmethod
    def list_assistants(cls):
        """
        List all registered assistants.

        Returns:
            List of registered assistant names.
        """
        return list(cls._registry.keys())
### react_assistant.py ###

from codehammer import CodeHammer
from codehammer import prompt_template as react_template
from langgraph.prebuilt import create_react_agent
from langgraph.checkpoint.memory import MemorySaver

from .assistant_registry import AssistantRegistry

# Define the assistant's prompt
prompt = react_template + "You are forbidden to call tools beyond the list provided."
memory = MemorySaver()


# Define the assistant builder function
def build_react_assistant(llm, base_dir):
    # Define the tools
    forge = CodeHammer(base_dir=base_dir)
    tools = forge.get_tools()
    return create_react_agent(llm, tools=tools, prompt=prompt, checkpointer=memory)


# Register the assistant
AssistantRegistry.register_assistant("react_assistant", build_react_assistant)
### __init__.py ###

### app.py ###
from flask import Flask, render_template, request, jsonify, session
from codeblacksmith.assistant import AssistantRegistry
from langchain_ollama import ChatOllama
import ollama
import re
import argparse
import uuid
import os

APP_DIR = os.path.dirname(os.path.abspath(__file__)) 
TEMPLATES_PATH = os.path.join(APP_DIR, "templates")
STATIC_PATH = os.path.join(APP_DIR, "static")

print("üî• Debug Info üî•")
print(f"üìÇ Current Working Directory: {APP_DIR }")
print(f"üìÅ Expected Templates Path: {TEMPLATES_PATH}")
print(f"üìÑ index.html Exists: {os.path.exists(os.path.join(TEMPLATES_PATH, 'index.html'))}")

# Initialize Flask with explicit template directory
app = Flask(__name__, template_folder=TEMPLATES_PATH, static_folder=STATIC_PATH)
app.secret_key = "supersecretkey"  # Required for session tracking

def get_available_models():
    """Returns a list of available models in Ollama."""
    try:
        return [m["name"] for m in ollama.list()["models"]]
    except Exception:
        return []

def format_response(response):
    """Formats assistant response to preserve code formatting."""
    response = re.sub(r"```python\n(.*?)\n```", r'<pre><code class="language-python">\1</code></pre>', response, flags=re.DOTALL)
    response = re.sub(r"```markdown\n(.*?)\n```", r'<pre><code class="language-markdown">\1</code></pre>', response, flags=re.DOTALL)
    return response.replace("\n", "<br>")

# Parse CLI arguments
parser = argparse.ArgumentParser(description="Start web assistant")
parser.add_argument("--model", required=True, help="Ollama model name")
parser.add_argument("--base-dir", required=True, help="Base directory for file operations")
args = parser.parse_args()

# Convert base_dir to an absolute path
BASE_DIR = os.path.abspath(args.base_dir)  # ‚úÖ Ensure correct path

# Change working directory to base_dir
os.chdir(BASE_DIR)  # ‚úÖ Ensure app has access to base_dir files

print(f"üîπ Server running with base directory: {BASE_DIR}")

# Initialize LLM
llm = ChatOllama(model=args.model)

# Retrieve assistant with `base_dir`
assistant_name = "react_assistant"
agent = AssistantRegistry.get_assistant(assistant_name, llm, BASE_DIR)  # ‚úÖ Pass absolute base_dir

@app.route("/")
def index():
    """Render the chat UI."""
    if "thread_id" not in session:
        session["thread_id"] = str(uuid.uuid4())  # ‚úÖ Generate unique thread_id

    return render_template(
        "index.html",
        models=get_available_models(),
        selected_model=args.model,
        base_dir=BASE_DIR,  # ‚úÖ Pass absolute base_dir
        thread_id=session["thread_id"]
    )

@app.route("/chat", methods=["POST"])
def chat():
    """Handle user input with thread_id for session tracking."""
    user_input = request.json.get("message")
    if not user_input:
        return jsonify({"error": "Empty input!"}), 400

    thread_id = session.get("thread_id", str(uuid.uuid4()))  # ‚úÖ Ensure thread_id persists

    inputs = {"messages": [("user", user_input)]}
    config = {"configurable": {"thread_id": thread_id}}  # ‚úÖ Include memory tracking

    response = agent.invoke(inputs, config=config)

    assistant_response = response["messages"][-1].content if "messages" in response else response.content
    return jsonify({"message": format_response(assistant_response)})

if __name__ == "__main__":
    app.run(debug=True, port=5000)
### cli.py ###
import argparse
import sys
import subprocess
import codehammer  # Import CodeHammer CLI commands
from .assistant.cli import register_commands as register_assistant_commands

#######################
# Command Forwarding  #
#######################
def forward_to_codehammer():
    """Forward unknown commands to CodeHammer."""
    command = ["codehammer"] + sys.argv[1:]
    result = subprocess.run(command, capture_output=True, text=True)
    print(result.stdout)
    if result.stderr:
        print(result.stderr, file=sys.stderr)
    sys.exit(result.returncode)

#######################
# Main CLI Entry Point#
#######################
def main():
    parser = argparse.ArgumentParser(description="CodeBlacksmith: AI & Code Management CLI")
    subparsers = parser.add_subparsers(dest="command", required=True)

    # Register CodeHammer CLI commands
    codehammer.cli.register_core_commands(subparsers)

    # Register Assistant Commands
    register_assistant_commands(subparsers)

    args, unknown_args = parser.parse_known_args()

    # If the command is from CodeHammer, forward it
    if args.command in ["tree", "file", "files", "files_recursive", "write", "combine", "clean_result"]:
        forward_to_codehammer()

    if hasattr(args, "func"):
        args.func(args)
    else:
        parser.print_help()
        sys.exit(1)

if __name__ == "__main__":
    main()
### setup.py ###
from setuptools import setup, find_packages

# ‚úÖ Find all sub-packages, including `assistant`
core_packages = find_packages(include=["codeblacksmith", "codeblacksmith.assistant"])

setup(
    name="codeblacksmith",
    version="0.0.1",  # Increment the version

    # ‚úÖ Only install core by default
    packages=core_packages,  

    include_package_data=True,

    install_requires=[
        "pytest",
        "pydantic",
        "langchain",
        "pathspec",
        "langchain_community",
        "langchain_ollama",
        "langgraph",
        "ollama",
        "duckduckgo-search",
        "flask",
        "codehammer==0.0.1"
    ],

    entry_points={
        "console_scripts": [
            "codeblacksmith=codeblacksmith.cli:main",
            "blacksmith=codeblacksmith.cli:main",
            "smith=codeblacksmith.cli:main",
        
        ],
    },

    description="A tool to combine code files into a single prompt",
    long_description=open("README.md", encoding="utf-8").read(),
    long_description_content_type="text/markdown",
    author="RG",
    url="https://github.com/RobinsonGarcia/CodePromptForge",
    author_email="rlsgarcia@icloud.com",
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.7",
)
